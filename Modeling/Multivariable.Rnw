
<<echo=FALSE,include=FALSE>>=
source('../include/setup.R')
opts_chunk$set( fig.path="figures/Multivariable-" ) 
if (!exists("..makingMaster..")) {
	set_parent('Master-Modeling.Rnw')
}
set.seed(123)
# for nicer printing for lm.
print.lm <- function(x,digits=4) {print(coef(x),digits=digits)} 
print.summary.lm <- function(x,digits=4) {print( coef(x),digits=digits)}
print.summary.glm <- function(x,digits=4){print(x$coefficients,digits=digits)}
print.anova <- function(x,digits=4) {print.data.frame(x,digits=digits)}
# Just for the t-test
print.ttest <- function(x,digits=4){
  paste("t = ",signif(x$statistic,digits),", df = ",
        signif(x$parameter,digits),", p-value = ",
        signif(x$p.value,digits),sep="")
}
@



\chapter*{Preface}

These notes present a strategy for teaching statistical modeling. Modeling is a way of making sense of the world by building a representation that is easy to explore and manipulate.  Modeling is a key capacity for dealing with complexity that's used in many ways.  Mathematical modeling refers to representations built out of mathematical objects, particularly functions.  Statistical modeling is an adaption of mathematical modeling to the extraction of information from data.

The strategy for teaching modeling is based around notation: a way of writing down forms of relationships among variables.  Coupled with modern computing, the notation comes to life.  The form of a relationship is translated by the computer into a fit that describes the data at hand.

The same notation that can describe simple relationships --- for example, groupwise means, in which a quantitative response variable is averaged separately in groups defined by a categorical explanatory variable --- can be extended to much richer relationships involving multiple explanatory variables.  What's key is to have a notation in which phrases like ``response variable'' and ``explanatory variable'' have a discernible identity.  Starting with that notation puts students in a good place on the road to learning about modeling.

The notes are part of a series on teaching with R, but they are not primarily about R.  Other notes in the series introduce R, discuss how to teach with R, and show how to carry out basic processes of statistical inference using conceptually simple operations implemented transparently in R.

If you don't already know R, we hope that you will think the commands are simple enough that you can use them yourself; that you can learn R by observation.  Our students appear to be able to do that.  

It is possible to implement the strategy presented here using other software.  It might even be possible to present it without using software at all.  But you always need some notation to communicate. The R computer notation is simple and concise, a rival for traditional mathematical notation, even as it extends to include operations not in the algebraic repertoire: sampling, randomization, iteration, etc. 

These notes are meant for teachers; this isn't a textbook for students.  Examples of materials for students, including a textbook \marginnote{DT Kaplan (2011) ``Statistical Modeling: A Fresh Approach'' 2nd ed., {\em Project MOSAIC}} are available at \url{www.mosaic-web.org/StatisticalModeling}.

\bigskip

We wish to acknowledge the support of the Howard Hughes Medical Institute, the W.M. Keck Foundation, and the US National Science Foundation (DUE-0920350).


<<include=FALSE>>=
require(mosaic)
require(fastR)
options(na.rm=TRUE)
@


\chapter{Graphics \& Formulas}

\marginnote{\texttt{require(mosaic)}\\
The R system provides a way for the software development community to add new functionality.  "Packages" are the standard way to deliver such software.  
The statement \texttt{require(mosaic)} loads in the \pkg{mosaic} package, which itself includes several other packages.  

The \pkg{mosaic} package provides many of the commands that are used in this book.

Once the package is loaded, it remains loaded for the rest of the R session.  

As you teach, you may find other packages, such as \pkg{fastR}, that give useful data sets or other capabilities.  These can be loaded using \function{require} in the same way.}



\marginnote{The \texttt{trebuchet} data set was collected by a high-school student, Andrew Pruim, as part of a Science Olympiad competition.  These data are included in the \pkg{fastR} package.  

A trebuchet is a device used for throwing projectiles. A heavy counter-weight pulls down the short end of the arm, rapidly accelerating a projectile hanging in a sack at the long end of the arm.  Introduced in medieval times, trebuchets were used as a siege weapons to destroy fortifications. 

\includegraphics[width=2in]{images/Trebuchet2.png}}

Visualization is sometimes called the gateway drug to statistics.  Since being a statistics addict is a good thing, it's worthwhile to start a statistics course with graphics:  scatterplots, box-and-whisker plots, histograms, etc.  In addition to conveying lay ideas of variation, graphics are close to the data themselves, compelling, and motivating.  Students see the advantage of using software; they couldn't make such graphics by hand.

As always in these notes, you'll be using, among other things, functions from the \texttt{mosaic} package.  You need to load this into R:
<<>>=
require(mosaic)
@
This will also load allied packages, such as \texttt{lattice} graphics.  Often, you will encounter methods or data that you want to use in others of the many packages available for R.  For instance, the next example involves a data set about a toy trebuchet available in the \pkg{fastR} package:
<<>>=
require(fastR)
@

\authNote{I'm simplifying the trebuchet data a bit, considering only \texttt{form=='a'}.
<<>>=
trebuchet = droplevels(subset(trebuchet, form=="a", drop=TRUE))
@
}
 

In R, these scatterplots and box-and-whisker graphs can be made with \function{xyplot} and \function{bwplot}, for instance:
<<ex1,fig.width=4,fig.height=2,fig.keep='all'>>=
xyplot(distance ~ projectileWt,data=trebuchet)
@ 
<<ex2,fig.width=4,fig.height=2,fig.keep='all'>>=
bwplot(distance ~ object,data=trebuchet)
@

Each of these examples involves two variables.  They are often called $y$ and $x$ by students.  Alternative names frequently encountered are ``vertical axis" and ``horizontal axis,'', ``dependent'' and ``independent'' variables, ``input'' and ``output.''  Among math teachers, ``ordinate'' and ``abscissa'' are sometimes the preferred terms.  Whatever you choose to call them --- we'll use \term{explanatory} and \term{response} variables when describing models, and \term{input} and \term{output} for functions --- the notation must make clear which variable is in which role.  

In R notation, a \term{formula} is an expression involving the \verb+~+ squiggle character --- also called ``tilde'' --- that provides slots for laying out how you want to relate variables: y versus x, what to break down by what.  

There's more going on here than just identifying which variable gets plotted on each axis: the formula in R provides a {\em modeling language} that gives an easy path from basic graphics and basic statistics to multivariable modeling.  The path is easy enough that it makes sense to start in that direction early: using the concepts, terminology, and techniques of modeling as a way of introducing statistics.  Here is what such a journey might look like.

\section{Formulas \& Basic Statistics}




\marginnote{It's tempting to show students shortcuts for looking at a data set, such as \function{summary}.
<<eval=FALSE>>=
summary(trebuchet)
@
This unfortunately encourages students to think that there is such a thing  as a mean or median of a dataset.  It's important to distinguish between variables and the datasets in which variables are contained.}

The move to quantitative statistics is straightforward, since the syntax is very much the same as with plots.  For instance
<<>>=
mean( distance,data=trebuchet)
sd( distance,data=trebuchet)
@
In each case, the variable of interest is named along with the data set in which that variable is included.  

Using \pkg{mosaic}, the formula interface works with these basic statistics in much the same way as for graphics, e.g:
<<>>=
mean( distance~object,data=trebuchet)
@

Eventually, the formula will be used to guide students to thinking about using explanatory variables to account for a response variable, in this example, explaining distance by object.  At this point, it works well to present this to students as ``breaking down'' the distance variable by the object variable, or just ``dividing up into groups.''  

\chapter{From Means to Models}

\marginnote{Remember to load the \texttt{mosaic} package:
<<eval=FALSE>>=
require(mosaic)
@
}

Students often understand means algorithmically in a way that can be expressed in plain English: ``add them up and divide by $n$.''  Such arithmetic is considered basic.

Techniques such as regression involve more complicated algorithms, ones that require algebraic notation.  But strip away the algorithm used to compute the parameters of the regression and consider just the statement of the regression operation itself.  Here is such a statement in R, modeling distance flown by the projectile as a function of the projectile's weight:
<<>>=
mod1 = lm(distance~projectileWt,data=trebuchet)
@
The syntax for constructing the model is very much the same as for \function{mean}.  But what is the output, what is the model itself?

It helps to have some vocabulary:
\begin{itemize}
\item A model of this sort links inputs, called ``explanatory variables,'' to an output, or ``response variable". \sidenote{Depending on the field, these may be called ``independent'' and ``dependent'' variables respectively.  You may want to avoid this nomenclature, since those terms are confusingly similar in pronunciation and are often mistaken to confer special status on the quantities themselves rather than their role in a model.} In the example above, \VN{distance} is the response variable while \VN{projectileWt} an the explanatory variable.
\item The model corresponds to a function, a mathematical representation of a relationship between an input and an output. 
\item The purpose of \function{lm} is to construct a function that is as close to the data as possible.  That is, \function{lm} finds the model that ``best'' fits the data.
\end{itemize}

\newthought{Students typically study functions} using a traditional algebraic notation, e.g. 
$$ y = 4 x + 2 .$$
Such notation doesn't emphasize the idea that $y$ is a function of $x$, that $x$ is the input and $y$ is the output. Indeed, there's nothing explicit to say that $y$ is a function at all; usually it's understood to be a variable.

To introduce students to a notation that makes such relationships explicit, \pkg{mosaic} provides \function{makeFun}:\marginnote{Math students get used to the convention of writing functions as $f(x)$ or $g(x)$.  You'll have to take care to remind them that $f$ is the function and $x$ is the name of the input. In these notes, we tend to use names like \texttt{f1} and \texttt{f2} for functions, and \texttt{mod1} and \texttt{mod2} for the statistical models.}
<<>>=
f = makeFun(4*x + 2 ~ x)
@

You can evaluate the function by specifying the inputs, for instance:
<<>>=
f(3)
@

Of course, functions such as this can be plotted in the ordinary way:
<<>>=
plotFun( f(x)~x, x.lim=range(-2,2))
@

The model, \texttt{mod1}, created earlier, is not exactly a function.\marginnote{You can see what's in \texttt{mod1} by giving it as a command, 
<<eval=FALSE>>=
mod1
@
This displays the model coefficients.  In addition to \function{makeFun}, you can use such operations as \function{rsquared}, \function{fitted}, and \function{resid}.} It's set up this way because there are several different types of information one might want to get from a model, not just the model function.  You can use \function{makeFun} to extract the model function from \texttt{mod1}: 
<<>>=
f1 = makeFun(mod1)
@
This use of \function{makeFun} reformats the information that's already in \texttt{mod1}  as a mathematical function in R. Once that's done, the function can be evaluated in the ordinary way, by specifying the inputs.

The function \texttt{f1} attempts to describe the relationship between the explanatory and response variables: between \VN{distance} and \VN{projectileWt}.


There are, of course, other possible functions that could be used to describe the relationship between \VN{distance} and \VN{projectileWt}.  Taking some measurements off the scatter plot, one might reasonably try a simple function, perhaps $y = 1900 - 30*x$.  No reason to use the names $y$ and $x$, however.  You can define the proposed function using the names in the data set:
<<>>=
f2 = makeFun( 1900 - 30*weight ~ weight)
@

Given these two functions, \function{f1} and \function{f2}, it's natural to ask, ``Which is which is better?''  Graphically, the answer is not so clear.  In the following plot, \function{f2} is drawn as the thicker line:
<<>>=
xyplot( distance ~ projectileWt, data=trebuchet)
plotFun(f1(projectileWt)~projectileWt, add=TRUE)
plotFun(f2(projectileWt)~projectileWt, add=TRUE, lwd=3)
@

\newthought{The usual way} to quantify how close each function comes to the data involves the residuals: the difference between the value as given by the model function and the actual data value.  For instance, for a \VN{projectileWt} of 60, the function values are:\marginnote{For models created fitted with \function{lm}, you can access the fitted values or residuals directly from the model structure, without needing to construct the function and evaluate it.  Use
<<eval=FALSE>>=
fitted(mod1)
resid(mod1)
@
}
<<>>=
f1(60)
f2(60)
@
You can be more systematic and evaluate the functions at every one of the data points:
<<tidy=FALSE>>=
trebuchet = transform(trebuchet,
                      f1resid=f1(projectileWt)-distance,
                      f2resid=f2(projectileWt)-distance)
@


%\marginnote{XX NOTE IN DRAFT: Include \function{sum} in the aggregating statistics.}

There is one residual for each row in the dataset.  There are several good ways to describe the size of the residuals, e.g. the standard deviation, variance, or the sum of squares.  Here is the standard deviation.
<<>>=
sd(f1resid,data=trebuchet)
sd(f2resid,data=trebuchet)
@
It appears that the residuals from \function{f1} are smaller.

\marginnote{Instructor Note: It's a good exercise to try out many different alternatives.  You won't be able to find any whose sum of square residuals are smaller than those produced by \function{f1}.  In this sense, \function{f1}, from the model constructed by \function{lm}, is the best of all possible straight-line models of the data.}

There are several ways to see the effect of \VN{projectileWt} on \VN{distance}.  The coefficients carry this information, but students need to be taught how to interpret them:
<<>>=
coef(mod1)
@
This says that for every 1 gm increase in the weight of the projectile, the predicted distance flown decreases by 14.3 cm.  (The units of the variables are given in the help file for the data, \texttt{help(trebuchet)}.)

Another way to see this same thing, perhaps a bit more obvious to the introductory student, is to evaluate the model function at two different weights, for instance:
<<>>=
f1(51) - f1(50) 
@

It's perfectly reasonable at this point to consider the extent to which the data dictate the best fit, and whether other possible straight-line functions, \marginnote{Resampling provides an accessible way to explore the sampling variation of a model.  See 
the mosaic package vignette on resampling for more examples.}even if their residuals are not as small as those from \function{f1}, are reasonable fits.  

For models created by \function{lm}, access to confidence intervals is provided by the model function.  Just ask for the interval, specifying whether you want an interval on the model value itself or on the predicted output for a given input.\marginnote{A confidence interval on a model value indicates how much the model value varies due to the particular random sample of data on which the model is based.  A prediction interval includes both the variation in the model value and the variation in the value of an individual case associated with typical residuals from the model value.}
<<>>=
f1(50,interval="confidence",level=0.95)
f1(50,interval="prediction",level=0.95)
@

Notice the absurdly wide prediction interval, which includes a non-physical negative distance, even though no backward-going launches are seen in the data itself.  This suggests that there's something wrong with the model for the purpose of making a prediction.  Let's go there.

\section{Multiple Inputs}

Consider why one might build a model of a trebuchet.  A practical application, if you are a medieval warrior, is to predict the distance travelled by a projectile: What weight is needed to reach the castle walls?

A competent trebuchet technician will tell you that there's another issue: How heavy is the counter-weight on the trebuchet?  You have the option of adding or taking away weight from the counter-weight in order to get your projectile on target.  

Fitting a relevant model is a matter of including the counter-weight (\VN{counterWt}) in the set of explanatory variables.  (It's also important that student Andrew Pruim collected the experimental data over a range of counter-weights.)  Here's a simple model:
<<tidy=FALSE>>=
mod2 = lm( distance~projectileWt+counterWt, data=trebuchet)
@

\marginnote{Once students understand that the name assigned to a function should be in the format $f$ rather than $f(x)$, you may want to start assigning more descriptive names to functions.  Notice that in writing about functions, we use the typographical notation \function{f} rather than the bare name \texttt{f}.  But remember than in assigning a name to a function not to use the parentheses after the function name.}

To extract the corresponding function, which will be a function of both \VN{projectileWt} and \VN{counterWt}, use \function{makeFun}.  We'll call the function \function{ballistic}:
<<>>=
ballistic = makeFun(mod2)
@

There are two input variables here, so an appropriate graphical display is a contour plot
<<fig.height=4,fig.width=4,tidy=FALSE>>=
plotFun( ballistic(projectileWt=x,counterWt=y) ~ x&y, 
        x.lim=range(10,70),y.lim=range(.5,3),
        xlab="Projectile (gms)",
        ylab="Counter Weight (kg)",
        main="Distance Thrown (cm)")
@

The first two lines of this do all the work, the remaining lines are just for setting labels.  Note that $x$ and $y$ are being used as the plotting variables.  $x$ is assigned to be the value of the projectile weight while $y$ is the value of the counter-weight.  It's helpful to use such explicit names for the input variables just to avoid accidentally reversing the meaning of the variables. With functions of more than one input it's easy to get things wrong.


It takes a bit of practice to learn to interpret such graphs, but it's worth the time.  Each contour shows the set of projectile weights and counter weights that can reach a given distance.  For instance, to reach 600 cm distance, the model suggests that one could use a projectile weight of 20 gm and a counterweight of about 1.25 kg, or one could up the projectile weight to 60 gm and increase the counterweight to a bit more than 2 kg.

Adding in the counter-weight as an explanatory variable puts creates a model that might be more useful to a trebuchet operator, but it also has an important meaning in terms of statistics.  The new variable can help account for some of the distance data, and in so doing can make the model a better fit.  

Many students will be unfamiliar with functions of two variables, simply because they don't encounter them in their mathematics classes.\marginnote{Conrad Wolfram offers a compelling critique of the traditional topics and order of math education in a TED talk: ``Teaching kids real math with computers.''}  It's a mistake to think that the standard mathematics curriculum has been designed to teach easy subjects first, and that subjects not encountered in the standard curriculum are somehow more difficult.  But you do have to orient your students to some of the basics of functions of two (or more) variables.

A good place to start is to ask your students to imagine themselves standing on a hillside.  Is the slope the same in all directions?  Many students will respond, ``yes.''  They have an intuitive notion of the gradient and are thinking that, at each point on the hillside, there is just one slope.  But, as skiers know, the hill is steep in some directions and not at all steep in others.  (Indeed, for every point on every hill, there is a direction where the landscape is flat.)  Looking back at the contour plot, ask the students whether the hill is steeper in the East-West direction (that is, along the $x$-axis), or in the North-South diretion (along the $y$ axis).  It's easy to find the slope in each direction, by taking a small step in that direction and finding the change in altitude.

For the trebuchet distance function, you can take such steps by varying one variable while holding the other constant.  For instance, what's the change in distance (according to the model) when changing the projectile weight by one gram while holding the counter-weight constant.  Pick a value for each variable and tweak projectile weight:

\begin{widestuff}
<<tidy=FALSE>>=
ballistic(projectileWt=51,counterWt=1) - ballistic(projectileWt=50,counterWt=1)
@
\end{widestuff}

This indicates that a 1 gm increase in projectile weight is associated with a decrease of 8.5 cm in the distance flown.  

On the other hand, increasing the counter weight is associated with an increase in distance:

\begin{widestuff}
<<tidy=FALSE>>=
ballistic(projectileWt=50,counterWt=2) - ballistic(projectileWt=50,counterWt=1)
@
\end{widestuff}

\noindent A 1 kg increase in the counter-weight is associated with an increase of 365 cm in the distance.

Show your students the points on the contour plot corresponding to these finite differences.\marginnote{Practice is important here.  The time invested will pay off handsomely.}  This helps them to understand why each input variable can be associated with a different connection to the output.  Once students understand this, it's easier for them to generalize to functions of more than two variables.  The key thing is to move away from functions of a single variable.

It's well worth observing that the two models, \texttt{mod1} and \texttt{mod2}, give different answers to the question of how the distance changes with a change in the projectile weight.  For \text{mod1} (translated into function \function{f1}) the answer is
<<>>=
f1(projectileWt=51)-f1(projectileWt=50)
@
But for \texttt{mod2} (translated into function \function{ballistic}) the change in distance is much less, as already seen:

\begin{widestuff}
<<tidy=FALSE>>=
ballistic(projectileWt=51,counterWt=1) - ballistic(projectileWt=50,counterWt=1)
@
\end{widestuff}

That the two functions give different answers can be confusing to students.  The difference comes about because the \function{ballistic} function allows you to hold the counter-weight constant while changing the projectile weight.  This is, of course, completely natural --- you can change the projectile weight without changing the counter-weight.  But the experiment happened to be done in such a way that only the lightest projectiles were used with the heaviest counterweights. \marginnote{Tip: The imbalance in this design --- that not all the levels of \VN{counterWt} were used for each level of \VN{projectileWt} --- is the norm for many observational studies.    Modeling makes it clear that imbalance introduces ambiguities that are related to covariation and can be dealt with in the same way.}
<<>>=
tally( ~ projectileWt + counterWt, data=trebuchet)
@
The result is that the light-weight projectiles flew, on average, much further than the heavier projectiles: partly because the projectiles were lighter and partly because the counter-weight was heavier.

If you're thinking, "Well, the experiment should have been done in a balanced way, with the same range of counter-weights used for each projectile," true enough.  But there experiment wasn't done this way and, as a result, counter-weight and projectile weight have been connected to one another.  To disconnect them, given the data, requires that both counter-weight and projectile weight be used to account for distance flown.

\section{More variables give a better fit}

Remember that your students have been raised mathematically in an environment where there is always a correct answer, sometimes easy to find and sometimes hard.  Many students conflate the difficulty of finding the answer with the quality of the fit --- data that show a clear pattern are easier to deal with than data that don't, and of course the fit is better for the data that show a clear pattern.

Your students will naturally think that fitting a function of two inputs is harder to fit than a function of a single input.  That's fair enough.  For a straight-line function of one variable, it's pretty easy to draw a plausible candidate and to use the techniques of high-school algebra to find the parameters: a slope and intercept.   But it's hard to draw a graph of a function of two variables, let alone use the eye to relate data to the parameters of the fitted function.

Given this focus on the difficulty of the problem, it becomes confusing to see that the function of two explanatory variables must fit the response variable better than a function of one variable.  So, take the time to demonstrate this.  A simple way to see that the two-variable model is a better fit than the one-variable model is to look at the size of the residuals.\marginnote{It's worthwhile to ask your students why the mean of the residuals of a fitted model is not a useful way to characterize their size.  Have them look at the mean residual from different models and figure out what's going on.}  As always, the way to quantify residuals is with their standard deviation, or variance, or sum of squares.
<<>>=
sd(resid(mod1))
sd(resid(mod2))
@

The two-variable model also provides a more reliable prediction:
<<tidy=FALSE>>=
f1( projectileWt=50, 
    interval="prediction")
ballistic( projectileWt=50,counterWt=2, 
           interval="prediction")
@
Notice how much narrower the prediction interval is for the two-variable model compared to the one-variable model.  

\marginnote{The observation that adding an explanatory variable to a model will reduce the size of the residuals provides a powerful segue to statistical inference.  The null hypothesis is often stated in terms of ``no difference'' or ``no effect.''  Perhaps better to state the null as ``no meaningful information in the explanatory variable(s).'' You can generate such uninformative explanatory variables using random numbers or shuffling. 
See the mosaic package resampling vignette for more information.}



\chapter{Functions with Categorical Variables}
\marginnote{Remember to load the \texttt{mosaic} package:
<<>>=
require(mosaic)
@
}

In terms of notation, there's not much difference between these two statements:

<<results="hide",tidy=FALSE>>=
vals = mean(distance~object,data=trebuchet)
mod  =   lm(distance~object,data=trebuchet)
@

The first calculates the groupwise means of \VN{distance}, with the groups defined by \VN{object}.
<<>>=
vals = mean(distance~object,data=trebuchet)
vals
@

The second statement fits a model that accounts for the variation in \VN{distance} by the variation in \VN{object}:

\begin{widestuff}
<<echo=FALSE>>=
tempw = options(width=80)
@
<<>>=
mod3 = lm(distance~object,data=trebuchet)
coef(mod3)
@
\end{widestuff}

\noindent As it happens, the model values from \texttt{mod} coincide directly with the values produced by \function{mean}. To see this, extract the function from the model:

\begin{widestuff}
<<>>=
g1 = makeFun(mod3)
@
\begin{multicols}{3}
<<>>=
g1("foose")
@
<<>>=
g1("golf")
@
<<>>=
g1("tennis ball")
@
\end{multicols}
\end{widestuff}

It may seem odd to your students that a word (or \term{character string}) can be used as the input to a function.  Remind them that a function is a machine that takes one or more inputs and produces an output.  There's no requirement that the inputs be numbers.

The function \function{g1} can be used to calculate a model value for each case in the \texttt{trebuchet} data and, from that, the residuals:

\begin{widestuff}
<<tidy=FALSE>>=
trebuchet = transform(trebuchet, g1resids = distance-g1(object))
@
\end{widestuff}

The size of the residuals can be described, as always, by their standard deviation:

\begin{widestuff}
<<>>=
sd(g1resids, data=trebuchet)
@
\end{widestuff}

To see what's special about the model values here --- that is, why the groupwise mean is special --- construct another function that assigns a specific value to each group.  For example, here's a function that says the golf balls go 300 cm, foose balls 500, tennis balls 100 and washers 300.

\begin{widestuff}%[h]
<<tidy=FALSE>>=
g2 = makeFun(
      switch(object,"golf"=300,"foose"=500,"tennis ball"=100,"big washerb"=300,NA) ~ object
     )
@
\end{widestuff}
This is not the traditional form for a function.  The function \texttt{g2} takes an input \texttt{object} and compares it to the names of the different kinds of balls.  It returns the associated value (or \texttt{NA} if there is no associated value), for instance:

\begin{widestuff}
\begin{multicols}{3}
<<>>=
g2("foose")
@
<<>>=
g2("golf")
@
<<>>=
g2("tennis ball")
@
\end{multicols}
\end{widestuff}

Which gives a better description of the distances flown by the various objects, \function{g1} or \function{g2}?  As previously, you can find the residuals from this model and compare them to the residuals from \function{g1}.

The function \function{g2} takes just a single character string.  To allow it to work on a vector of character strings, you can \term{vectorize} it:\authNote{Any way to avoid this?}%
<<>>=
g2 = Vectorize(g2)
@

\begin{widestuff}
<<tidy=FALSE,warning=FALSE>>=
trebuchet = transform(trebuchet, g2resids = distance - g2(object))
sd(g1resids,data=trebuchet)
sd(g2resids,data=trebuchet)
@
\end{widestuff}

\function{g2} produces larger residuals than the function \function{g1} produced by fitting to data.  The values of the parameters of the fitted model minimize the size of the  residuals (as measured by the sum of squares or the standard deviation) compared to any other parameter values.

\chapter{From $r$ to $R^2$}

\marginnote{Remember to load the \texttt{mosaic} package:
<<>>=
require(mosaic)
@
}

The correlation coefficient, $r$, figures prominently in many introductory statistics courses.  It's typically presented as quantifying the relationship between two quantitative variables.  That $r$ represents {\em a} relationship is true enough.  But it's misleading to say that $r$ describes {\em the} relationship. $r$ quantifies the quality of fit of a straight-line function, but that's hardly the only relationship even when just two variables are involved. 


$r$ is not as useful as its prominence suggests.  It doesn't handle multiple explanatory variables.  It doesn't handle models with categorical variables.  It doesn't handle nonlinear relationships.  It doesn't even give an ``effect size,'' just a measure of the quality of fit to a straight-line model.

On the other hand, $R^2$ is much more generally useful.  If you are teaching modeling, it makes  sense to introduce $R^2$ as early as possible.  The way to do this is {\bf not} to treat $R^2$ as the square of $r$.  Such a development inherits all the deficiencies of $r$.  Instead, go back to a basic question: How does the model account for the variation in the response variable.

Earlier, the standard deviation was used as a way to quantify the size of residuals.  Let's use it now to quantify the size of variation in the response variable.  For the \VN{distance} variable in the \texttt{trebuchet} data, this is:
<<>>=
sd( distance,data=trebuchet)
@
Now consider the size of the variation in the model values from the various models we have fitted --- \texttt{mod1}, \texttt{mod2}, and \texttt{mod3} from which were extracted the model functions \function{f1}, \function{ballistic}, and \function{g2}:
<<>>=
sd( f1(projectileWt),data=trebuchet)
sd( ballistic(projectileWt,counterWt),data=trebuchet)
sd( g1(object),data=trebuchet)
@
In every case, the size of variation in the fitted model values is {\bf less} than the size of the response variables.  Where's the rest of the variation?  In the residuals.

A fitted model \term{partitions} variation between that accounted for by the model and that which remains unaccounted for.  Measure the variation accounted for using the variation in the fitted model values; measure the rest of the variation using the residuals.  For instance, for \texttt{mod1}:
<<>>=
sd(fitted(mod1))
sd(resid(mod2))
@
It would be nice if the two parts of the variation added up to the whole:
<<>>=
sd(distance,data=trebuchet)
sd(fitted(mod1))+sd(resid(mod2))
@
Alas, that's not exactly true.  The reason, however, is that the standard deviation is not the natural statistic for measuring variation, even if it is the one used in introductory statistics.  Instead, use the square of the standard deviation --- the variance:
<<>>=
var(fitted(mod1))
var(resid(mod1))
var(distance,data=trebuchet)
@
Note that the sum of variances of the fitted model values and the residuals add up exactly to the variance of the response variable.  
<<>>=
var(fitted(mod1))+var(resid(mod1))
@

The situation here is analogous to one your students have encountered before: the Pythagorean theorem: $A^2 + B^2 = C^2$.  It's the square-length of the sides of a right triangle that add in a nice way, not the lengths themselves.  Similarly, the variances add in a nice way, not the standard deviations. 
%\marginnote{For more about the analogy between geometry and model fitting, see \cite{bock-velleman}, \cite{kaplan-fresh} and \cite{saville}.}

The $R^2$ statistic on a model describes what fraction of the variance in the response variable is accounted for by a model.  You can calculate it directly:
<<>>=
var(fitted(mod1))/var(distance,data=trebuchet)
@
For convenience, you can extract the $R^2$ statistic directly from the model, just as you can extract the model function, the fitted values and residuals:
<<>>=
rsquared(mod1)
@

An important question is whether $R^2$ can be used to compare different model designs to decide which is best.  For instance, $R^2$ from the groupwise-mean model \texttt{mod3} is somewhat larger than for \texttt{mod1}:
<<>>=
rsquared(mod3)
@
Does this mean that \texttt{mod3} is better than \texttt{mod1}?  That will turn out to be a productive route to studying statistical inference.  But before heading in that direction, let's expand the set of models that students can build and interpret.

\chapter{Combinations of categorical and quantitative variables}

\marginnote{Remember to load the \texttt{mosaic} package:
<<>>=
require(mosaic)
@
}

So far, you have encountered these sorts of models:
\begin{itemize}
\item One quantitative explanatory variable.
\item One categorical explanatory variable.
\item Two quantitative explanatory variables.
\end{itemize}

All of these sorts models were constructed with the same syntax and all of them fit into the same framework: explanatory and response variables, fitted model values, residuals, $R^2$.  The syntax and framework extend to more complicated models.  You can add in more explanatory variables using exactly the same syntax.

You can also add in \term{interactions} among explanatory variables.

To illustrate, consider world records in the 100 meter freestyle swimming event as they have changed over the years.  Plot these separately for the two sexes.

\begin{widestuff}
<<swim-data,fig.width=6>>=
xyplot(time ~ year | sex, data=SwimRecords)
@
\end{widestuff}

It's evident from the data that, for both sexes the records are improving over time. (How could they not?  That's the nature of a world record.)  The pattern is so clear that one hardly needs a model to interpret it.  But, to display the syntax of models, let's do so anyways.

The record \VN{time} depends on both \VN{sex} and \VN{year}, but it's your choice what explanatory variables to include in a model.  Here are three plausible models:
<<>>=
swim1 = lm(time~sex,data=SwimRecords)
swim2 = lm(time~year,data=SwimRecords)
swim3 = lm(time~year + sex, data=SwimRecords)
@
To plot out the model function, first extract the function from the model:
<<>>=
s1 = makeFun(swim1)
s2 = makeFun(swim2)
s3 = makeFun(swim3)
@

The first model doesn't include \VN{year}.  Still, to graph the model function on the axes, you need to include \VN{year} in the plotting statement. 

\begin{widestuff}
<<fig.width=6>>=
<<swim-data>>
plotFun(s1(sex="F")~year,add=TRUE)
plotFun(s1(sex="M")~year,add=TRUE,lty="dotdash")
@
\end{widestuff}

The function \function{s1} doesn't depend on \VN{year}, so the graphs of the function are flat with respect to \VN{year}.  The function does have \VN{sex} as an input and you can see in the graph how the function values for females (thick line) differ from those for males (thin line).

Now consider \function{s2}, which depends on \function{year} but not \function{sex}.

\begin{widestuff}
<<fig.width=6>>=
<<swim-data>>
plotFun(s2(year)~year,add=TRUE)
plotFun(s2(year)~year,add=TRUE,lty="dotdash")
@
\end{widestuff}

The functions are the same for males and females, of course, so they overlie one another on the graph.

Including both \VN{sex} and \VN{year} in the model produces a function that depends on both variables:

\begin{widestuff}
<<fig.width=6>>=
<<swim-data>>
plotFun(s3(year=year,sex="F")~year,add=TRUE)
plotFun(s3(year=year,sex="M")~year,add=TRUE,lty="dotdash")
@
\end{widestuff}

You might be surprised to see that the graph of the function for males is parallel to that for females.  That's because there was nothing in the model design that produces a different slope with respect to \VN{year} for females and males: the two lines must therefore be parallel. 

Including such a difference in a model is a matter of including an \term{interaction term} between \VN{sex} and \VN{year}:

\begin{widestuff}
<<fig.width=6>>=
swim4 = lm(time~year+sex+year:sex,data=SwimRecords)
s4 = makeFun(swim4)
<<swim-data>>
plotFun(s4(year=year,sex="F")~year,add=TRUE)
plotFun(s4(year=year,sex="M")~year,add=TRUE,lty="dotdash")
@
\end{widestuff}

\newthought{Interactions are confusing}; they imply a ``difference of differences.'' \marginnote{For students who have been exposed to the algebra of functions of two variables, it may be helpful to point out that the model $x + y + x:y$ corresponds to the polynomial $f(x,y) = a_0 + a_1 x + a_2 y + a_3 x y$.  The coefficient on the interaction term is $a_0$.  This corresponds to the mixed partial derivative, $\partial^2 f/\partial x \partial y$.  This is the calculus equivalent of ``difference of differences.''} Students tend to want to interpret the word ``interaction'' as meaning that one variable affects the other.  This is not quite right.  An interaction describes how the effect of one variable on the response is modulated by the other variable.  For example, the interaction between \VN{sex} and \VN{year} tells how the relationship between \VN{year} and world-record \VN{time} differs for the two sexes.  You see that interaction in the graph as different slopes for the fitted lines for the two sexes.

Another, way to describe the interaction is that the relationship between \VN{sex} and world-record \VN{time} is changing over the years.  You can see that from the changing vertical distance between the lines for females and males. Both these ways of describing the interaction --- how the relationship between \VN{sex} and \VN{time} is modulated over the years, and how the relationship between \VN{year} and \VN{time} is different for the two sexes --- are equivalent.  Given that the slopes of the two lines is different, the vertical distance between the two lines is going to change.

\newpage

\section{Example: The Genetic Component of Human Height}

The world-record swim-time data is ordered enough that it's easy to draw a satisfactory functional approximation by hand.  That makes it easier for students to visualize how different model terms set the ``shape'' of the function.  But students may wonder what statistics has to do with it.

\newthought{To place things more firmly in a statistical context},\marginnote{Francis Galton, ``Correlations and their measurement, chiefly from anthropometric data'' (1889) {\em Nature} 39:238, and ``Regression towards mediocrity in hereditary stature'' (1886) {\em Journal of the Anthropological Institute of Great Britain and Ireland} 15:246-263.  For a commentary and access to further background on the data, see James Hanley, `` `Transmutting' Women into men: Galton's family data on human stature'' (2004) {\em American Statistician} 58(3):237-243)} consider the data collected by Francis Galton in 19th century London.  Galton was interested in exploring the heritability of biological traits, in particular the relationship between the heights of parents and their full-grown, adult children.  These data played an important part in the development of the correlation coefficient and regression toward the mean  

A man of his era, Galton focused on the heights of sons.  Here are both sexes of children, plotted out against the mother's height:

\begin{widestuff}
<<galton-data,fig.width=6>>=
xyplot(height ~ mother | sex, data=Galton)
@
\end{widestuff}

From the graph alone, it's obvious that height differ from males to females and there is a slight tendency that a taller mother is associated with taller children.  Here's a model that includes an interaction term between the child's sex and the mother's height:

\begin{widestuff}
<<fig.width=6>>=
<<galton-data>>
hmod1 = lm(height~mother*sex,data=Galton)
h1 = makeFun(hmod1)
plotFun(h1(mother=m,sex="F")~m,add=TRUE)
plotFun(h1(mother=m,sex="M")~m,add=TRUE,lty="dotdash")
@
\end{widestuff}

What's new in this example is that a specific line can be judged as the best fit to a cloud of data.  Certainly a student could do this by hand, but they would likely have little confidence that their particular line was best.  Certainly, the precision with which one might draw a line by hand wouldn't justify drawing lines of different slopes for the males and females.  Indeed, it remains to be seen whether the interaction term is contributing much to the model.  That sort of question provides a segue to statistical inference.  (See below.)

What's the father's role in this.  In a scatter plot, it's impossible to use both the father and the mother along on the $x$-axis, one has to choose.  There are some tricks, for example creating panels for different intervals of the father's height, but it's hard to gain much quantitative insight from the graphic.

\begin{widestuff}
<<galton-father,fig.width=5,fig.height=5,tidy=FALSE>>=
xyplot(height ~ mother | cut(father,breaks=2) + sex, data=Galton)
@
\end{widestuff}

Instead, consider a model that uses both mother's and father's height (and the child's sex) to account for the child's height.  For now, leave off the interaction terms; you can return to those later with some statistical inference tools in hand: 

\begin{widestuff}
<<fig.width=5,fig.height=5,tidy=FALSE>>=
hmod2 = lm(height~mother+father+sex,data=Galton)
h2 = makeFun(hmod2)
<<galton-father>>
plotFun(h1(mother=m,sex="F")~m,add=TRUE)
plotFun(h2(mother=m,father=64,sex="F")~m,add=TRUE,lty="dotdash")
plotFun(h2(mother=m,father=74,sex="F")~m,add=TRUE,lty="dotted")
@
\end{widestuff}

The dotted and dot-dashed lines show the mother+father model values for two different heights of father (just for female children).  For comparison, the solid line shows the model with just the mother. That the lines are different for the two different heights of father shows the association between father's height and child's height, for each given mother's height.  


\section{Partial Change}

``All things being equal'' is an everyday phrase.   In the Galton height data, for instance, one can examine the association of \VN{mother}'s height with child's \VN{height}, holding the other things constant, e.g., the \VN{father}'s height and the child's \VN{sex}.  Given a model function, this \term{partial change} is easy to calculate: look at the difference in child's model height for two different values of the mother's height, while holding \VN{father} and \VN{sex} constant.  For instance,

\begin{widestuff}
<<tidy=FALSE>>=
h2(mother=66,father=68,sex="F") - h2(mother=65,father=68,sex="F")
@
\end{widestuff}

Similarly, one can examine the partial change with respect to \VN{father} and with respect to \VN{sex}:

\begin{widestuff}
<<tidy=FALSE>>=
h2(mother=66,father=68,sex="F") - h2(mother=66,father=67,sex="F")
h2(mother=66,father=68,sex="F") - h2(mother=66,father=68,sex="M")
@
\end{widestuff}

\newthought{The reason to call these differences ``partial'' change} is by analogy with partial derivatives in calculus: the change with respect to one variable holding the other constant.

Of course, for the continuous variables --- \VN{mother} and \VN{father} --- one can calculate the partial derivative itself.  This would be appropriate for students who are familiar with derivatives, but it is not essential that one consider any sort of limiting process as in calculus.  The important point is that the change in model output can be considered with respect to each of the input variables individually.

The partial change is a straightforward measure of \term{effect size}.  The intellectual question is what quantities to hold constant.  Answering this requires some expert knowledge.  Students have considerable expertise, even if it's just about common sense matters.  For instance, it's a form of expert knowledge to know that the mother's height doesn't affect the sex of the child.  So in considering the effect size of mother's \VN{height}, it's sensible to hold \VN{sex} constant. 

Occasionally, it makes sense to consider the change while varying two or more variables simultaneously.  As an example with a simple mechanism, consider trying to predict a person's wage based on their education and job experience.  The \texttt{CPS85} data contains information from the Current Population Survey that can be used for this purpose.  First, build a model with both education and experience as explanatory variables (and whatever covariates you think appropriate), e.g.,
<<>>=
wmod = lm(wage~educ+exper,data=CPS85)
@
Wage here is in dollars per hour (in 1985).  To look at the ``effect'' of a college education, you might examine the partial difference with education varying to 16 years from 12 years while experience is being held constant at, say, 10 years. (Twelve years of education corresponds to a high-school graduate.)
<<>>=
w1 = makeFun(wmod)
w1(educ=16,exper=10)-w1(educ=12,exper=10)
@
Judging from this, the four years of extra education is associated with a predicted increase in wage of \$3.70 per hour.  

But hold on.  The four years of extra education comes by decreasing work experience by those four years (if experience is ), so the proper comparison is not a partial change in one variable alone, but a simultaneous, compensatory change in education and experience:
<<>>=
w1(educ=16,exper=10)-w1(educ=12,exper=14)
@

% EXERCISES: From the total versus partial sheets.

\section{Covariates}

Often, the purpose of a model is to describe the relationship between the response and a single explanatory variable, e.g. how does blood pressure respond to a drug versus placebo.  Almost always, though, there are additional explanatory variables in which there is little or no direct interest but which may play an important role in the relationship.  The term \term{covariate} is used to designate such variables.  Of course, in a mathematical sense, covariates are just ordinary explanatory variables.  The word ``covariate'' simply signals the modeler's lack of direct interest in them. 

Other related terms are \term{confounders} or \term{lurking variables}.  These terms properly suggest the vulnerability of the conclusions drawn from a model to unknowns or to known variables not included in a model.  But whenever a variable is known and measured, it should be considered as a candidate to be included in a model.  Unthinkingly to leave a covariate out of a model is burying your head in the sand.  

It's traditional to point to the situation of an experiment.  In one form of experiment, identifiable confounders are held constant by design.  In another form, both identifiable and unidentified conditions are balanced by randomized assignment (since the coin flip of randomization will tend to balance out other factors, on average).  Understandably, statistics textbooks warn about the perils of drawing conclusions about causation from observational data.  Such warnings follow the conventions of a mathematical emphasis on proof. 

There are lessons to be drawn from other fields, however.  In epidemiology, for instance, important conclusions to guide action need to be drawn from imperfect, observational data. 

To illustrate, consider a news story  (``Coffee and Smoking: A Daily Habit Of Green Tea Or Coffee Cuts Stroke Risk", by Allison Aubrey, NPR - March 15, 2013) reporting on research findings published in the American Heart Association journal {\em Stroke}.  The main result: a daily habit of coffee or tea drinking is associated with a decrease of 20\% in stroke risk.  The news story puts this in a historical context: 
\begin{quotation}
% ... recent studies have linked a regular coffee habit to a range of benefits  from a reduced risk of Type 2 diabetes to a protective effect against Parkinson's disease.

It's interesting to note how much the thinking about caffeine and coffee has changed.

In the 1980s, surveys found that many Americans were trying to avoid it; caffeine was thought to be harmful, even at moderate doses.

One reason? Meir Stampfer of the Harvard School of Public Health says back then, coffee drinkers also tended to be heavy smokers. And in early studies, it was very tough to disentangle the two habits.

``So it made coffee look bad in terms of health outcomes,'' says Stampfer.

But as newer studies began to separate out the effects of coffee and tea, a new picture emerged suggesting benefits, not risks.

Researchers say there's still a lot to learn here --- they haven't nailed down all the mechanisms by which coffee and tea influence our health. Nor have they ruled out that it may be other lifestyle habits among coffee and tea drinkers that's leading to the reduced risk of disease.
\end{quotation}

Austin Bradford Hill was an epidemiologist and statistician --- the president of the Royal Statistical Society who succeeded Fisher.  He pioneered randomized clinical trials, taken as the gold standard for inferring causation in medicine. Hill's famously offered nine viewpoints for guiding causal inference.  Number eight is ``Experiment'': \marginnote{AB Hill, ``The environment and disease: association or causation?'' {\em Proceedings of the Royal Society of Medicine} (1965) 58:295-300}
\begin{quotation}
Occasionally, it is possible to appeal to experiment, or semi-experimental evidence.  For example, because of an observed association some preventive active is taken.  Does it in fact prevent?  The dust in the workshop is reduced, lubricating oils are changed, persons stop smoking cigarettes.  Is the frequency of the associated events affected?  Here the strongest support for the causation hypothesis may be revealed.
\end{quotation}
The prior seven are strength, consistency, specificity, temporality, biological gradient, plausibility, and coherence, each of which garners a longer explanation by Hill than experiment.  Experiment may be the simplest and most compelling, but experiment is not always possible or available.

If students are to operate in a world where causal inferences will be drawn from non-experimental data, they certainly need to be aware of confounding and lurking variables, the ecological fallacy, etc.  But they also need to have the tools to attempt to untangle confounding.  Multivariable modeling provides a straightforward way to do this.

DL Guber \marginnote{Deborah Lynn Guber, ``Getting what you pay for: the debate over equity in public school expenditures'' (1999), {\em Journal of Statistics Education} 7(2).} presents a nice example of untangling confounding in the context of achievement and expenditure in public education.  Drawing on the 1997 {\em Digest of Education Statistics}, Guber assembled a data set of state-by-state averages that can be used to relate school expenditures to SAT.  It's easy to construct a model.
<<>>=
mod1 = lm( sat ~ expend, data=SAT)
summary(mod1)
@

Judging from this model, expenditures are negatively associated with SAT scores.  The relationship is statistically significant.  Commenting on the association (if not the statistical significance), well-known editorial columnist George Will \marginnote{GF Will, ``Meaningless money factor'' {\em Washington Post}, 12 Sept. 1993} points to Senator Pat Moynihan's humorous observation of a positive correlation between scores on standardized math tests and distance of the states' capitals from the Canadian border, a correlation that's stronger than that seen between test scores and per-pupil expenditures.  Will goes on:
\begin{quotation}
In a 1992 study ... Paul Barton argues that a more powerful measure of school quality than the pupil-teacher ratio is the parent-teacher ratio.  ... The proportions of children in single-parent families vary substantially among the states, so some conclusions are suggested by data such as: In a recent year North Dakota had the nation's second-highest proportion of children in two-parent families and the highest math scores.  The District of Columbia ranked last on the family composition scale and next to last in test scores.
\end{quotation}
% Full text here: http://www.isds.duke.edu/courses/Spring06/sta101.1/homework.dir/GeorgeWillEditorial.pdf

While Moynihan's distance-from-Canada variable is not meant to be taken seriously, the parent-teacher ratio variable is a serious contender.  It's hardly possible to do an experiment to vary the parent-teacher ratio.  Without experiment, what's left?

Will writes:
\begin{quotation}
The fact that the quality of schools correlates more positively with the quality of the families from which children come to school than it does with education appropriations will have no effect on the teachers unions' insistence that money is the crucial variable.
\end{quotation}

What's wrong here is the idea of the ``crucial variable.''  Teachers' unions are understandably concerned with education appropriations, just as editorial columnists are with the structure of families.  That these variables are ``crucial'' reflects the interests of the modelers --- it's perfectly feasible for a model to include both variables.  Rather than identifying a single variable as crucial and looking for an association with that variable to the exclusion of all other explanatory variables, it's more appropriate to construct a model with multiple variables.  The issue is confounding, not cruciality. 

Returning to the SAT data, consider one simple confounder, \VN{frac}, the proportion of students in each state who take the SAT.  In upper Mid-west states like North Dakota, many college-bound students take the ACT rather than the SAT; SATs tend to be taken by students heading out of state, who are often higher-scoring.  In many states, only a small fraction of students take the SAT, and these students also tend to be higher-scoring. So use \VN{frac} as a covariate: \marginnote{Confounding occurs any time a covariate is correlated with an explanatory variable.  The term \term{Simpson's paradox} is used to identify situations when the coefficient on the explanatory variable of interest changes sign when a covariate is introduced into a model.  It's called a ``paradox'' because the sign change isn't anticipated by intuition.  But a change in coefficient is an inevitable result of the correlation of a covariate with an explanatory variable.  ``Paradox'' shouldn't be interpreted as ``rare'' or ``unlikely.''  Confounding is a perfectly ordinary situation.}  
<<>>=
mod2 = lm(sat ~ expend+frac,data=SAT)
summary(mod2)
@

Taking into account the covariate \VN{frac}, the relationship between expenditures and test scores is positive and statistically significant. Such a substantial change in the value of a coefficient when including a covariate is a sign of confounding.

More than one covariate can be included in a model, of course.  The models themselves won't sort out what causes what, but they provide a framework for having such a debate.  

\subsection{Example: What's a Fireplace Worth?}

Statistician Richard De Veaux has shared a data set on house prices in Saratoga Springs, NY.  In addition to the sales price of the house, the there is a variable indicating whether or not the house has a fireplace:
<<message=FALSE>>=
houses = fetchData("SaratogaHouses.csv")
median( Price ~ Fireplace, data=houses )
summary(lm(Price ~ Fireplace, data=houses))
@
Houses with a fireplace are about \$70,000$\pm$9000 more expensive than houses without.  But this doesn't mean that a fireplace is worth \$70,000.  Houses with fireplace are have other traits that distinguish them from houses without, for example, they tend to be larger.
<<>>=
summary(lm(Living.Area ~ Fireplace, data=houses))
@
It seems sensible to build a model that takes such confounding into account by including \VN{Living.Area} as a covariate:
<<>>=
summary(lm(Price ~ Fireplace+Living.Area, data=houses))
@
This model puts the value of a fireplace at about \$10,000$\pm$8000.  Notice that the difference between estimates made using the different models is much larger than the margin of error for either model.  This illuminates a point that it's often difficult to convey to students: a margin of error has to do with sampling variation, not with proximity to the ``true'' value.


\chapter{Statistical Inference}

\marginnote{Remember to load the \texttt{mosaic} package:
<<>>=
require(mosaic)
@
}

Statistical inference --- confidence intervals, hypothesis testing, etc. --- is often presented as being about data.  Data are undeniably central, but it's misleading to distract attention from the context in which the data are being analyzed.  

\section{Hypothesis Testing}

To someone familiar with t-tests, one-way ANOVA, etc., the previous sentence may seem odd.  What context is there to asking whether two groups are different or whether there is a difference between multiple groups.  But consider the matter as it would be expressed in modeling notation:

\begin{description}
\item[Two-way t test.]  Are the means of two different groups different?  In the modeling notation, this corresponds to the significance of the coefficient on a grouping variable with two levels, e.g.
<<>>=
mod1 = lm(time~sex,data=SwimRecords)
coef(mod1)
@
\item[One-way ANOVA.] Are the means of several different groups different.  In the modeling notation, this corresponds to the ability of a categorical variable with more than two levels to account for variation in the response variable.

\begin{widestuff}
<<>>=
mod2 = lm(wage~sector,data=CPS85)
coef(mod2)
@
\end{widestuff}
\end{description}

There are at least two strong advantages to presenting inference in terms of the modeling notation.
\begin{itemize}
\item Modeling allows covariates to be introduced.  There's not much point in worrying about the second digit in a p-value (that is, is $p<0.05$) when the first digit might be strongly influenced by covariates.
\item Modeling emphasizes the common logic of statistical inference, allowing students to handle multiple different settings with the same logic.
\end{itemize}
George Cobb \marginnote{GW Cobb (2007). ``The Introductory Statistics Course: A Ptolemaic Curriculum?'' {\em Technology Innovations in Statistics Education}, 1(1).} has described the logic of statistical inference as the ``Three Rs'': randomize, repeat, reject.  To illustrate how this applies to models, let's construct a simple t- or ANOVA-type test on the two models given above. \marginnote{More examples of the Three Rs are given in the \pkg{mosaic} resampling vignette.}

First, you need a \term{test statistic}.  A convenient one is $R^2$.  

Next, you need to randomize the explanatory variable and calculate the test statistic under randomization.  Doing this many times gives a sampling distribution under the Null Hypothesis.  Comparing the actual (non-randomized) value of the test statistic then allows a p-value to be extracted.

As a specific example, consider a simple model with the swim-records data and $R^2$ as the test statistic: 
<<>>=
mod1 = lm(time~sex,data=SwimRecords)
test.stat = rsquared(mod1) # test statistic
test.stat
@
Now, just for demonstration, carry out one randomization trial and find the test statistic:
<<>>=
# One randomization
rsquared(lm(time~shuffle(sex),data=SwimRecords))
@
It's worthwhile when working with students to repeat the above line several times to show that the test statistic varies.  Then you can move on to having the computer repeat the trials many times and collect the results:

\begin{widestuff}
<<>>=
s = do(1000)*rsquared(lm(time~shuffle(sex),data=SwimRecords))
@
\end{widestuff}

Finally, to ask how often the random trials produce a test statistic at least as strong as that observed in the original data:

\begin{widestuff}
<<>>=
tally( ~result >= test.stat, data=s, format="count")
@
\end{widestuff}

\newcommand{\lsim}{\mathrel{\hbox{\rlap{\lower.55ex \hbox{$\sim$}} \kern-.3em \raise.4ex \hbox{$<$}}}}
The result: in none of the 1000 trials did the randomization produce a test statistic as strong as that observed in the original data.  This suggests a p-value $\sim 0.001$.

It's exactly the same structure for the one-way ANOVA test.

\begin{widestuff}
<<>>=
mod2 = lm(wage~sector,data=CPS85)
test.stat2 = rsquared(mod2) # test statistic
s2 = do(1000)*rsquared(lm(wage~shuffle(sector),data=CPS85))
tally( ~result >= test.stat2, data=s2, format="count")
@
\end{widestuff}

Once students see the structure of a hypothesis test, it's easy for them to switch to interpretation mode: using the built-in normal-theory calculations to generate p-values:

\newpage
<<>>=
anova(mod1)
anova(mod2)
@

It may seem odd to get an ANOVA report instead of a t-test.  But a t-test\marginnote{For sticklers: It's the equal variance t-test that's equivalent to ANOVA.  But when have you ever seen an unequal variance t-test that performs better than an equal-variance t-test on ranks?  The unequal variance t-test is a mathematical nicety, contributing little or nothing to statistical insight.} is the same thing as ANOVA:
<<ttest,results="hide">>=
t.test(time~sex,data=SwimRecords,var.equal=TRUE)
@
<<echo=FALSE>>=
cat(print.ttest(t.test(time~sex,data=SwimRecords,var.equal=TRUE)))
@


You may ask, doesn't it matter that the simulation-based p-values are different from the normal-theory p-values.  Doesn't this mean that the simulations are wrong?  The answer is that there's not right or wrong here, just what is reasonable.  How precisely do you need to know a p-value?  Is there any value to reporting more than two digits of a p-value?

What's more, the p-values generated by the normal theory are based on assumptions that may be unwarranted, so extreme precision in the reported value is itself unwarranted.  As many statistical educators recognize, there are \term{distributional assumptions}.  Arguably more important are the covariates.  The statistical methods you teach should allow students to include covariates as appropriate.  The hypothesis tests implemented by \function{shuffle} or by the theory behind \function{anova} and the regression-table generator \function{summary} extend automatically and easily to models with covariates.

This also provides a new context for conducting hypothesis tests:  ``Does including this covariate improve the model for the purpose?''  If the data provide little evidence for the non-null role of a covariate in the context of the other explanatory variables, perhaps best to leave it out of the model.

\section{Confidence Intervals}

Cobb's randomize and repeat logic of statistical inference applies to the construction of confidence intervals as well.  In this setting, rather than shuffling variables, resample with replacement on a case-by-case basis, so that each individual case preserves the authentic relationship among its variables.  To illustrate, here's a resampling based calculation of the standard error on a model of the coefficient on \VN{projectileWt} in a model of the trebuchet throw-distance:
<<tidy=FALSE>>=
tmod = lm(distance~projectileWt+counterWt,data=trebuchet)
s = do(1000)*lm(distance~projectileWt+counterWt,
                data=resample(trebuchet))
sd(projectileWt,data=s)
@

After seeing the randomization-based logic of the construction of confidence intervals, it's easy to shift to the normal-theory results:
<<>>=
summary(tmod)
@

Indeed, one might as well introduce the confidence interval itself, rather than the standard error, 
<<>>=
confint(tmod)
@

Of course, the confidence intervals depend on the structure of the model.  Add in more covariates, add in interaction or transformation terms, and the confidence intervals may get smaller or larger.  Explaining how this happens (see, e.g., Chapter 12 in {\em Statistical Modeling: A Fresh Approach}) is arguably more important that working students through the algebraic details of a calculation that does not apply in the situation that's appropriate for answering the question at hand.  Use resampling to establish the logic of confidence intervals, then have students explore the interpretation of confidence intervals and the factors on which they depend.

\chapter{Keeping Models in Proportion}

\marginnote{Remember to load the \texttt{mosaic} package:
<<>>=
require(mosaic)
@
}

A conventional introduction to statistics has methods relating to means (t-tests, one-way and two-way ANOVA) and to proportions (p-tests) and to counts ($\chi^2$ tests).  Quantities such as $\sqrt{p(1-p)/N}$ and $\sum \frac{(obs-exp)^2}{exp}$ are almost iconic.  The modeling approach might at first seem to have nothing to do with proportions and counts, but that's not true.  Indeed, proportions and counts fit in well with the modeling framework.  Modeling unifies seemingly disparate methods under one roof.

To illustrate, consider a simple set of data about the health effects of smoking: \texttt{Whickham}.  The data are from a survey of women in Whickhamshire, UK.  The women were surveyed to find (among other things) their ages and whether they smoke.  A follow-up was conducted 20 years later, at which time it was recorded whether the woman was still alive.

Here's the count of women, broken down by the alive/dead outcome:
<<>>=
tally(~outcome, data=Whickham)
@
And the proportion in each group:
<<>>=
tally(~outcome, data=Whickham, format="proportion")
@
It's a conventional problem in intro stats to give a standard error on the proportion, typically using the Wald formula.  For the fraction who have died, the standard error is: $\sqrt{p(1-p)/N} = \sqrt{0.2808 ( 1 - 0.2808)/1314} = 0.012$.  This gives a 95\% margin of error of 0.024 and therefore a Wald confidence interval of $0.2808 \pm 0.0235 = [0.257,0.305]$.

Another way to get the same proportions is to take the mean of the Yes/No variable that indicates whether the person has died:
<<>>=
mean(~outcome=="Dead", data=Whickham)
@

Now remember that the model with just an intercept produces model values that are the same as means, so the proportion can also be calculated as:
<<>>=
mod0 = lm(outcome=="Dead" ~ 1, data=Whickham)
coef(mod0)
@
Now the apparatus of modeling can be applied, for instance to generate the 95\% confidence interval:
<<>>=
confint(mod0)
@

\newpage

A more typical application involves the difference between two proportions, say the fraction of the smokers who died versus the fraction of non-smokers.
<<>>=
tally(~outcome|smoker, data=Whickham, format="proportion")
@
It looks like the smokers are a little {\em less} likely to have died.

In modeling language, we're interested in whether the outcome depends on the explanatory variable \VN{smoker}:
<<>>=
mod1 = lm(outcome=="Dead" ~ smoker, data=Whickham)
summary(mod1)
@
The smokers have a lower death rate. The difference is significant.

In a non-modeling course, this question might be addressed by a p-test, or by a $\chi^2$ test.  For example:
<<>>=
counts = tally(~outcome & smoker, data=Whickham, margins=FALSE)
counts
chisq.test(counts)
@

The p-value is similar to that from regression.  This is not to say that the $\chi^2$ p-value is right, even though the counts are large.  The Fisher exact test gives yet another p-value:
<<>>=
fisher.test(counts)
@

Should you be concerned about the differences between 0.0025 and 0.0030?  There's no practical meaning to the difference.  However, there is a very large practical importance to the covariate \VN{age}, which has not been included in the analysis to this point.

It's easy enough to build a model that incorporates \VN{age}:
<<>>=
mod2 = lm(outcome=="Dead" ~ smoker+ age, data=Whickham)
summary(mod2)
@
This model suggests that the null hypothesis cannot be rejected.  The p-value on \VN{smoker} is about 0.6.  There is little point in worrying about the second significant digit of a p-value on the difference of counts or proportions when the first digit of the p-value depends on a covariate.

It's worth mentioning that a linear model, while reasonable, is not the right sort of model to build to model proportions.  Better to use a logistic regression model.  This involves a few new concepts (odds, log-odds) and a new R command, but the similarities of logistic regression to linear modeling are larger than the differences.  For reference, here's the logistic regression version of just-smoker model and the age-adjusted model:
<<tidy=FALSE>>=
mod3 = glm(outcome=="Dead"~smoker,
           data=Whickham, family="binomial")
summary(mod3)
mod4 = glm(outcome=="Dead"~smoker+age,
           data=Whickham,family="binomial")
summary(mod4)
@

\end{document}


\chapter{EXERCISES}

These are just sketches for exercises.  Where to deploy them?

\paragraph{One}. Why square instead of taking the absolute value?  There's nothing wrong with mean absolute value as a way of describing the sum of the residuals. Construct the model function where the value is the median of each group.  That function will have a mean absolute value of differences that is as small as any other possible function.

\paragraph{Two}. Do the grand mean.  It's not so easy to think about what the input is.  There is no input.  In this sense, groupwise means are easier to conceive of than grand means.

\paragraph{Three}. Show that \VN{object} gives a perfect prediction of \VN{projectileWt} in the \texttt{trebuchet} data. Does \VN{object} contribute anything beyond \VN{projectileWt} in explaining \VN{distance}?

\shipoutProblems

\bibliographystyle{alpha}
\bibliography{../include/USCOTS}



